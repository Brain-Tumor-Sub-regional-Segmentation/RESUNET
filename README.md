### ResUNet

ResUNet, a semantic segmentation model inspired by deep residual learning and U-Net, combines the strengths of both architectures for accurate pixel-wise predictions.

#### ResNets

Deep Residual Networks (ResNets) address the vanishing/exploding gradient problem by introducing residual blocks with skip connections. These connections enable the network to learn identity mappings, facilitating the training of very deep networks.

#### Architecture Main Parts

1. **Encoder (Contracting Path)**:
   - ðŸ“‰ Captures essential image features through multiple encoder blocks with pre-activated residual blocks.
   - ðŸ”„ Each block's output serves as a skip connection for the corresponding decoder block.
   - Stride-2 convolutions in some layers reduce spatial dimensions.
   - Each encoder block includes pre-activated residual blocks.

2. **Bottleneck Layer (Bridge)**:
   - ðŸ”— Connects the encoder and decoder, further processing feature maps with a pre-activated residual block.

3. **Decoder (Expansive Path)**:
   - ðŸ”¼ Upscales features for pixel-wise predictions.
   - Uses upsampling layers and concatenates with skip connections from the encoder.
   - Each decoder block includes pre-activated residual blocks.

4. **Output Layer**:
   - ðŸŽ¯ A 1Ã—1 convolutional layer with sigmoid activation produces the final segmentation mask.

ResUNets excel in medical imaging tasks such as organ and tumor segmentation.

ðŸ“„ Read the Deep Residual U-Net paper [here](https://arxiv.org/pdf/1711.10684) on ArXiv for a formal introduction.
